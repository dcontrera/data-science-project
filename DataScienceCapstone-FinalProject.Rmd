---
title: "Data Science Capstone - Final Project"
author: "Daniel Contrera"
date: "12/29/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, cache = FALSE, eval = TRUE, message = FALSE, warning=FALSE}
library(profvis)
mlines <- 10000
nmax <- 4
lowfreq <- 1
lowfreq1 <- 5
lowfreqn <- 1
# sparsity <- .9995
# sparsity <- 1
```
In the following chunk we read each subset
```{r, cache = TRUE, eval = TRUE, message = FALSE, warning=FALSE}
filenames <- c(
    "Coursera-SwiftKey/train/en_US.twitter.txt",
    "Coursera-SwiftKey/train/en_US.blogs.txt",
    "Coursera-SwiftKey/train/en_US.news.txt"
)
lines_file <- c()
for (f in filenames) {
    con <- file(f, "r")
    max_lines <- mlines
    lines_file <- c(lines_file, readLines(con, max_lines))
    close(con)
}
max_lines <- length(lines_file)
```

```{r, cache = TRUE, eval = TRUE, message = FALSE, warning=FALSE}
filenames <- c(
    "Coursera-SwiftKey/test/en_US.twitter.txt",
    "Coursera-SwiftKey/test/en_US.blogs.txt",
    "Coursera-SwiftKey/test/en_US.news.txt"
)
lines_file_test <- c()
for (f in filenames) {
    con <- file(f, "r")
    max_lines_test <- 500
    lines_file_test <- c(lines_file_test, readLines(con, max_lines_test))
    close(con)
}
max_lines_test <- length(lines_file_test)
```

Then:

- keep the letters, `'` and spaces

- replace double spaces with a single one

- delete spaces at the beginning or end

- lowercase

```{r, cache = TRUE, eval = TRUE}
inds <- 1:max_lines
lines <- lines_file[inds]
lines[inds] <- gsub("((?![a-zA-Z' ]).)*", "", lines[inds], perl = TRUE)
lines[inds] <- gsub("  +", " ", lines[inds])
lines[inds] <- gsub("^ | $", "", lines[inds])
lines[inds] <- tolower(lines[inds])
```

```{r, cache = TRUE, eval = TRUE}
inds_test <- 1:max_lines_test
lines_test <- lines_file_test[inds_test]
lines_test[inds_test] <- gsub("((?![a-zA-Z' ]).)*", "", lines_test[inds_test], perl = TRUE)
lines_test[inds_test] <- gsub("  +", " ", lines_test[inds_test])
lines_test[inds_test] <- gsub("^ | $", "", lines_test[inds_test])
lines_test[inds_test] <- tolower(lines_test[inds_test])
```

## Prediction Model

```{r, eval = TRUE, message = FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
```

### n-gram

```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(tm)
library(RWeka)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
text <- VCorpus(VectorSource(lines))
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
tw <- Reduce(paste, lines_test)
tw <- strsplit(tw, "'")[[1]]
tw <- Reduce(paste, tw)
tw <- strsplit(tw, " ")[[1]]
twu <- unique(tw)
```

```{r, eval=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
sw <- Reduce(paste, sample(lines, 10))
sw <- strsplit(sw, "'")[[1]]
sw <- Reduce(paste, sw)
sw <- strsplit(sw, " ")[[1]]
swu <- unique(sw)
mean(swu %in% twu)
```

```{r nGramTokenizer, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
# tdm <- TermDocumentMatrix(
#     text, 
#     control=list(
#         tokenize = function (x) NGramTokenizer(
#             x, 
#             control = Weka_control(min = 1, max = nmax)
#         ),
#         wordLengths=c(1, Inf)
#     )
# )
mergeVectors <- function (v1, v2) {
    v3 <- v1
    # nv1 <- names(v1)
    # nv2 <- names(v2)
    # for (i in 1:length(v2)) {
    #     if (nv2[i] %in% names(v3)) {
    #         v3[nv2[i]] <- v3[nv2[i]] + v2[nv2[i]]
    #     } else {
    #         v2i <- v2[nv2[i]]
    #         names(v2i) <- nv2[i]
    #         v3 <- c(v3, v2i)
    #     }
    # }
    d1 <- data.frame(f = v1)
    d1$name <- names(v1)
    d2 <- data.frame(f = v2)
    d2$name <- names(v2)
    douter <- rbind(d1, d2[!(d2$name %in% d1$name), ])
    dinner <- rbind(d1, d2[d2$name %in% d1$name, ])
    dinner <- dinner %>% group_by(name) %>% summarise(f = sum(f)) %>% ungroup()
    dm <- rbind(dinner, douter)
    v3 <- dm$f
    names(v3) <- dm$name
    v3
}
lchunk <- 1000
nchunks <- length(lines) %/% lchunk
tfs <- c()
# for (i in 1:(nchunks + 1)) {
    # print(paste("nchunk:", i, "/", nchunks))
    # if ((1 + (i-1)*lchunk) <= min((i*lchunk), length(lines))) {
        tfsi <- termFreq(
            # lines[(1 + (i-1)*lchunk):min((i*lchunk), length(lines))], 
            lines, 
            control = list(
                tokenize = function (x) NGramTokenizer(
                    x, 
                    control = Weka_control(min = 1, max = nmax)
                ),
                wordLengths=c(1, Inf)
            )
        )
        tfs <- c(tfs, tfsi)
        # tfs <- mergeVectors(tfs, tfsi)
    # }
# }
rm(tfsi)
```

```{r, cache=FALSE, eval=FALSE, message=FALSE, warning=FALSE}
ft <- findFreqTerms(tdm, lowfreq = lowfreq)
tf <- tdm[ft,]
if (sparsity < 1) {
    tf <- removeSparseTerms(tf, sparsity)
}
tsft <- sapply(ft, function (f) tm_term_score(tf, f, sum))
# mtf <- as.matrix(tf)
# rs <- rowSums(as.matrix(tdm[ft,]))
# rs <- rowSums(mtf)
# rm(mtf)
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
tsft <- tfs[tfs >= lowfreq]
rm(tfs)
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# ng <- data.frame(ng=names(rs), freq = rs)
ng <- as.data.frame(tsft)
rm(tsft)
# ng <- as.data.frame(tfs)
names(ng) <- "freq"
ng$ng <- row.names(ng)
sp <- strsplit(as.character(ng$ng), " ")
ng$len <- sapply(sp, length)
```


```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
ng <- ng %>% filter((len == 1 & freq >= lowfreq1) | (len > 1 & freq >= lowfreqn))
```

```{r, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
pWords <- function (sampleText, wordsRef) {
    swt <- Reduce(paste, sample(sampleText, 10))
    swt <- strsplit(swt, "'")[[1]]
    swt <- Reduce(paste, swt)
    swt <- strsplit(swt, " ")[[1]]
    swut <- unique(swt)
    mean(swut %in% wordsRef)
}
pWordsUnk <- c()
for (i in 1:100) {
    pWordsUnk <- c(pWordsUnk, pWords(lines_test, ng$ng[ng$len == 1]))
}
pUnk <- 1 - mean(pWordsUnk)
```


```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# freqUnk <- sum(sapply(sp, length) == 1)
freqUnk <- round(sum(ng$freq[ng$len == 1]) * pUnk)
unkRow <- data.frame(ng = "UNK", freq = freqUnk, row.names = "UNK", len = 1)
ng <- rbind(ng, unkRow)
# ng$freq <- ng$freq - 1

ng <- ng %>% arrange(-freq)
ngn <- ng %>% group_by(len) %>% summarise(n = n(), freq = sum(freq))
ng$perc <- ng$freq
# ngtest <- ng %>% mutate(perc = perc / ngn$n[ngn$len == len])
# ngtest <- ng %>% mutate(perc = perc / ngn$n[ngn$len == len])
for (i in 1:length(ngn$len)) {
    ng$perc[ng$len == ngn$len[i]] <- ng$perc[ng$len == ngn$len[i]] / (ngn$freq[i]**0.7) / (ngn$len[i]**1.3)
}
ng <- ng %>% arrange(-perc)
# ng$perc_cum <- ng$perc
# for (i in 2:length(ng$perc_cum)) {
    # ng$perc_cum[i] <- ng$perc_cum[i - 1] + ng$perc_cum[i]
# }
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
sp <- strsplit(as.character(ng$ng), " ")
ng$len <- sapply(sp, length)
ng$hist <- sapply(sp, function(x) {
        if (length(x) == 1) {
            "UNK"
        } else {
            Reduce(paste, x[1:(length(x) - 1)])
        }
    }
)
ng$y <- sapply(sp, function(x) Reduce(paste, x[length(x):length(x)]))
rm(sp)
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
nh <- ng %>% 
    select(hist, freq) %>%
    group_by(hist) %>%
    summarise(freq = sum(freq)) %>%
    arrange(-freq)
```

Discount

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
fhf <- table(nh$freq)
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# discount
d <- function (cis, fhf) {
    ncis <- fhf[as.character(cis)]
    ncis1 <- fhf[as.character(cis + 1)]
    ncis <- replace(ncis, which(is.na(ncis)), 1)
    ncis1 <- replace(ncis1, which(is.na(ncis1)), 1)
    # count good turing
    cgtis <- (cis + 1) * ncis1/ncis
    # ifelse(cs == 0, fhf[1]/sum(fhf), cgts/cs)
    ds <- c()
    for (i in 1:length(cis)) {
        di <- ifelse(cis[i] == 0, fhf[1]/sum(fhf), cgtis[i]/cis[i])
        ds <- c(ds, di)
    }
    ds
}
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# count ngram
c_hist <- function (hist, nh) {
    nh[nh$hist %in% hist, ]$freq
}
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# beta
# b <- function (hist, k, ng, nh, fhf) {
b <- function (hists, k, ngfb, nhb, fhf) {
    if (length(ngfb) == 0) {return(1)} 
    # words <- ng$y[ng$hist == hist]
    # hwords <- paste(hist, words)
    # C <- ng$freq[ng$ng %in% hwords & ng$freq > k]
    # C <- ng$freq[ng$ng %in% hwords]
    # C <- (ng %>% filter(ng %in% hwords) %>% select(freq))$freq
    # C <- (ng %>% filter(hist == hist) %>% filter(y %in% words) %>% select(freq))$freq
    Cis <- ngfb
    betas <- c()
    for (i in 1:length(hists)) {
        Ch <- nhb$freq[nhb$hist == hists[i]]
        dcis <- d(Cis, fhf)
        sums <- dcis * Cis/Ch
        summ <- sum(sums)
        if (summ < 0) {summ <- 0}
        else if (summ > 1) {summ <- 1}
        beta <- 1 - summ
        betas <- c(betas, beta)
    }
    betas <- c(betas, 1)
    betas
}
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
Pbo <- function (wis, hists, k, ngp, nhp, fhf, ahs, Cwh, n_words, ih = 1) {
    if (length(hists[ih]) == 1 & hists[1] == "") {
        ph <- wis
    } else {
        ph <- paste(hists[ih], wis)
    }
    Cwis <- c()
    # for (j in 1:length(ph)) {
    for (j in 1:length(wis)) {
        # if (ph[j] %in% ng$ng) {
        if (hists[ih] %in% ngp$hist & wis[j] %in% ngp$y) {
            # Cwis <- c(Cwis, ng$freq[ng$ng == ph[j] & ng$freq > k])
            Cwi <- ngp$freq[ngp$hist == hists[ih] & ngp$y == wis[j] & ngp$freq > k]
            Cwis <- c(Cwis, ifelse(length(Cwi) > 0, Cwi, 0))
        } else {
            Cwis <- c(Cwis, 0)
        }
    }
    dws <- d(Cwis, fhf)
    # ah <- a(hist, k, ng, nh, fhf)
    ws <- strsplit(hists[ih], " ")[[1]]
    if (length(ws) > 1) {
        hist2 <- Reduce(paste, ws[2:length(ws)])
        if (hist2 %in% ngp$ng) {
            Cwh2 <- ngp$freq[ngp$ng == paste(hist2)]
        } else {
            Cwh2 <- ngp$freq[ngp$ng == "UNK"]
        }
        # beta2 <- b(hist2, k, ngh$freq, nh, fhf)
        # ah2 <- a(hist2, k, ngp, nhp, fhf, beta2, n_words)
        # ah2 <- a(hists, k, ngp, nhp, fhf, betas, n_words, ih = ih + 1)
        # pBack <- Pbo(wis[i], hist2, k, ngp, nhp, fhf, ah2, Cwh2, n_words)
        # p <- c(p, ah * pBack)
    }
    p <- c()
    for (i in 1:length(wis)) {
        if (Cwis[i] > k & !(length(hists[ih]) == 1 & hists[ih][1] == "")) {
            p <- c(p, dws[i] * Cwis[i] / Cwh)
        } else {
            if (length(ws) > 1) {
                pBack <- Pbo(wis[i], hists, k, ngp, nhp, fhf, ahs, Cwh2, n_words, ih = ih + 1)
                p <- c(p, ahs[ih] * pBack)
            } else {
                # n_words <- sum(ngp$freq[ng$len == 1])
                if (wis[i] %in% ngp$ng) {
                    p <- c(p, dws[i] * ngp$freq[ngp$ng == wis[i]]/n_words)
                } else {
                    p <- c(p, dws[i] * ngp$freq[ngp$ng == "UNK"]/n_words)
                }
            }
        }
    }
    p
}

Pboh <- function (wis, hist, k, ngph, nhph, fhf, n_words) {
    if (!(hist %in% ngph$hist)) {
        hist <- "UNK"
    }
    words <- ngph$y[ngph$hist == hist]
    hwords <- paste(hist, words)
    sp <- strsplit(hist, " ")[[1]]
    hists <- c()
    for (i in 1:length(sp)) {
        # hists <- c(hists, Reduce(paste, sp[(length(sp) + 1 - i):length(sp)]))
        hists <- c(hists, Reduce(paste, sp[i:length(sp)]))
    }
    # if (!("UNK" %in% hists)) {
        # hists <- c(hists, "UNK")
    # }
    # ngfh <- ng$freq[ng$hist %in% hists & ng$freq > k]
    ngphh <- ngph[ngph$hist %in% hists & ngph$freq > k, ]
    # nghfhw <- ngh$freq[ngh$ng %in% hwords]
    if (hist %in% ngph$ng) {
        Cwh <- ngph$freq[ngph$ng == paste(hist)]
    } else {
        Cwh <- ngph$freq[ngph$ng == "UNK"]
    }
    # beta <- b(hist, k, ngphh$freq, nhph, fhf)
    # ah <- a(hist, k, ngph, nhph, fhf, beta, n_words)
    betas <- b(hists, k, ngphh$freq, nhph, fhf)
    ahs <- a(hists, k, ngph, nhph, fhf, betas, n_words, ih = (length(hists) + 1):1)
    # Pboh <- c()
    # for (j in 1:length(wis)) {
        Pboh <- Pbo(wis, hists, k, ngph, nhph, fhf, ahs, Cwh, n_words, ih = 1)
    # }
    Pboh
}
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# alpha
a <- function (hists, k, nga, nha, fhf, betas, n_words, ih = 1, as = c()) {
    if (length(as) == 0) {
        as <- vector(mode = "numeric", length = length(ih)) - 1
    } 
    # for (i in 1:length(hists)) {
    for (i in ih) {
        if (betas[i] > 0) {
            words <- nga$y[nga$hist == hists[i]]
            ws <- strsplit(hists[i], " ")[[1]]
            # if (is.list(ws) & length(ws) > 1) {
            # if ((length(ws) - i) > 0) {
            if (length(ws) > 1) {
                # hist2 <- Reduce(paste, ws[2:length(ws)])
                hist2 <- hists[i + 1]
                if (hist2 %in% nga$ng) {
                    Cwh2 <- nga$freq[nga$ng == paste(hist2)]
                } else {
                    Cwh2 <- nga$freq[nga$ng == "UNK"]
                }
                # beta2 <- b(hist2, k, nga$freq, nha, fhf)
                # beta2 <- betas[i + 1]
                # ah2 <- a(hists, k, nga, nha, fhf, betas, n_words, ih = i + 1)
                Pbos <- Pbo(words, hist2, k, nga, nha, fhf, as, Cwh2, n_words, ih = i + 1)
            } else {
                # if (any(ng$ng %in% wi)) {
                #     Pbos <- ng$freq[ng$ng %in% wi]
                # } else {
                #     Pbos <- ng$freq[ng$ng == "UNK"]
                # }
                # Pbos <- numeric(length(words))
                Cwh2 <- nga$freq[nga$ng == "UNK"]
                words <- nga$y[nga$hists[i] == "UNK"]
                if (length(words) > 100) {
                    words <- words[1:100]
                } else {
                    words <- "UNK"
                }
                if (length(nga$y) > 100) {
                    ngap <- nga[1:100, ]
                } else {
                    ngap <- nga
                }
                if (length(nha$hists[i]) > 100) {
                    nhap <- nha[1:100, ]
                } else {
                    nhap <- nha
                }
                Pbos <- Pbo(words, "UNK", k, ngap, nhap,
                            fhf, -1, Cwh2, n_words, ih = 1)
            }
            summ <- sum(Pbos)
            if (summ < 0) {summ <- 0}
            else if (summ > 1) {summ <- 1}
            ai <- betas[i]/(1 - summ)
            if (ai < 0) {ai <- 0}
            else if (ai > 1) {ai <- 1}
            else if (is.na(ai)){
                ai <- 1
            }
        } else {
            ai <- 0
        }
        as[i] <- ai
    }
    as
}
```

```{r, cache=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
predictWord <- function (wi, hist, k, ng, nh, fhf, hlen = 1:3) {
    sp <- strsplit(tolower(hist), " ")[[1]]
    if (length(hlen) == 1 & hlen[1] == -1) {hlen <- 1:length(sp)}
    # else if (length(hlen == 1)) {hlen <- 1:hlen}
    # for (h in -1:-(hlen)) {
    for (h in -hlen) {
        hi <- Reduce(paste, sp[h:-1 + length(sp) + 1])
        # Pbos <- Pbo(wi, hi, k, ng, nh, fhf)
        # preds <- wi[which(Pbos == max(Pbos))]
        preds <- predMarks[predMarks$hist == hi, ]
        # print(hi)
        # print(Pbos)
        print(preds)
        # ng %>% filter(ng %in% preds) %>% select(freq) %>% first() %>% print()
        # print(Pbos[which(Pbos == max(Pbos))]/norm(as.matrix(Pbos)))
    }
}
k <- 0
```

```{r eval=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# profvis({
    # m <- matrix(nrow = length(nh$hist), ncol = length(ng$y[ng$len == 1]))
    # m <- matrix(nrow = 5000, ncol = 5000)
    # m <- matrix(nrow = 1000, ncol = 1000)
    save_m <- TRUE
    # hists <- nh[nh$hist != "UNK", ]$hist[1:nrow(m)]
    # wis <- ng$y[ng$y != "UNK" & ng$len == 1][1:ncol(m)]
    hists <- nh$hist[1:nrow(m)]
    wis <- ng$y[ng$len == 1][1:ncol(m)]
    rownames(m) <- hists
    colnames(m) <- wis
    format(object.size(m), units = "auto")
    # ngf <- ng[1:ncol(m), ]
    # nhf <- nh
    # nhf <- nh
    n_words <- sum(ng$freq[ng$len == 1])
    # for (i in 1:nrow(m)) {
    for (i in 201:nrow(m)) {
        print(paste(i, "/", nrow(m), sep = ""))
        # print(nh$hist[i])
        # hwords <- paste(hists[i], wis)
        # ngfh <- ngf$freq[ngf$ng %in% hwords & ngf$freq > k]
        sp <- strsplit(hists[i], " ")[[1]]
        histsis <- c()
        for (j in 1:length(sp)) {
            histsis <- c(histsis, Reduce(paste, sp[j:length(sp)]))
        }
        histsis <- c(histsis, "UNK")
        nhh <- nh[nh$hist %in% histsis, ]
        ngh <- ng[ng$hist %in% histsis, ]
        m[i, ] <- Pboh(wis, hists[i], k, ngh, nhh, fhf, n_words)
        # m
        if (save_m & (i %% 100 == 0)) {
            print("Saving...")
            save(m, file = "m.RData")
        }
    }
# }, interval = 0.005)

```

```{r eval=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
profvis({
    m <- matrix(nrow = 2, ncol = 100)
    rownames(m) <- nh$hist[1:nrow(m)]
    colnames(m) <- ng$y[ng$len == 1][1:ncol(m)]
    # format(object.size(m), units = "auto")
    # ngf <- ng[1:ncol(m), ]
    nhf <- nh
    ngf <- ng
    # nhf <- nh
    for (i in 1:nrow(m)) {
        m[i] <- Pboh(
            ng$y[ng$len == 1], 
            nh$hist[i], 
            k, 
            ngf, 
            nhf, 
            fhf
        )
    }
}, interval = 0.005)

```


```{r, cache=TRUE, eval = TRUE, message=FALSE, warning=FALSE}
predMark <- function (wi = "", hist, m) {
    if (!any(hist %in% rownames(m))) {hist <- "UNK"}
    rinds <- match(hist, rownames(m))
    if (!any(wi %in% colnames(m))) {wi <- "UNK"}
    if (length(wi) == 1 & wi[1] == "UNK") {
        cinds <- 1:ncol(m)
    } else {
        cinds <- match(wi, colnames(m))
    }
    mhw <- as.matrix(m[rinds, cinds])
    rownames(mhw) <- rownames(m)[rinds]
    colnames(mhw) <- colnames(m)[cinds]
    preds <- unlist(apply(mhw, 1, which.max))
    predTops <- apply(mhw, 1, function (x) {data.frame(Pbo = x) %>% slice_max(n = 5, order_by = Pbo, with_ties = FALSE) %>% rownames()})
    pred <- data.frame(hist = hist[1:length(preds)], pred = colnames(m)[preds])
    if (nrow(predTops) > 1) {
        for (i in 2:nrow(predTops)) {
            pred[ , paste("pred", i)] <- predTops[i, ][1:length(preds)]
        }
    }
    pred
}
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
predMarks <- predMark("", rownames(m), m)
predMarks
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
npreds <- 5
yt <- data.frame(row.names = rownames(predMarks), stringsAsFactors = FALSE)
# names(yt) <- 1:npreds
for (i in 1:length(predMarks$hist)) {
# for (i in 1:11) {
    # rg <- regexpr(paste0("(^", as.character(predMarks$hist[i]), ".* ", ")|", "( ", as.character(predMarks$hist[i]), ".* )"), lines)
    # rg <- regexpr(paste0("^(", as.character(predMarks$hist[i]), ") .* .*"), lines)
    # rg <- regexpr(paste0(" ", as.character(predMarks$hist[i]), " .* "), lines)
    rg <- regexpr(paste0(" ", as.character(predMarks$hist[i]), " [a-zA-Z]+ {1}"), lines)
    ini <- (rg[rg > 0] > 1) * 1
    yths <- substr(lines[rg > 0], rg[rg > 0] + ini, rg[rg > 0] + attr(rg, "match.length")[rg > 0] - 2)
    yths <- strsplit(yths, " ")
    yths <- lapply(yths, function (x) {x[length(x)]})
    ythsf <- table(unlist(yths))
    ythsf <- sort(ythsf, decreasing = TRUE)
    nmax <- min(npreds, length(ythsf))
    # max <- which.max(ythsf)
    maxs <- ythsf[1:nmax]
    yts <- names(maxs)
    if (length(yts) < npreds) {
        yts <- c()
        for (i in max(1, length(yts)):npreds) {
            yts <- c(yts, "")
        }
    }
    # for (j in 1:length(yts)) {
        yt <- rbind(yt, as.data.frame(t(yts))) 
    # }
}
names(yt) <- paste("yt ", 1:npreds)
predMarks <- cbind(predMarks, yt)
```

```{r, cache=TRUE, eval = TRUE, message=FALSE, warning=FALSE}
accuracy <- -1
preds <- as.character(predMarks[predMarks$`yt  1` != "", "pred"])
preds[preds == "UNK"] <- predMarks[predMarks$`yt  1` != "" & predMarks$pred == "UNK", "pred 2"]
yts <- predMarks[predMarks$`yt  1` != "", paste("yt ", 1:npreds)]
includes <- c()
for (i in 1:length(preds)) {
    include <- preds[i] %in% as.character(unlist(yts[i, ]))
    includes <- c(includes, include)
}
pred <- cbind(preds, yts, includes)
accuracy <- mean(pred$includes)
accuracy
predMarks[predMarks$`yt  1` != "", "acc"] <- pred$includes
```

```{r, cache=TRUE, eval = TRUE, message=FALSE, warning=FALSE}
predPhraseMatrix <- function (wis, hist) {
    if (!("UNK" %in% wis)) {
        wis <- c("UNK", wis)
    }
    n_words <- sum(ng$freq[ng$len == 1])
    sp <- strsplit(hist, " ")[[1]]
    histsis <- c()
    for (j in 1:length(sp)) {
        histsis <- c(histsis, Reduce(paste, sp[(length(sp) + 1 - j):length(sp)]))
    }
    histsis <- c("UNK", histsis)
    mPhrase <- matrix(nrow = length(histsis), ncol = length(wis))
    rownames(mPhrase) <- histsis
    colnames(mPhrase) <- wis
    nhh <- nh[nh$hist %in% histsis, ]
    ngh <- ng[ng$hist %in% histsis, ]
    for (i in 1:length(histsis)) {
        # print(paste(i, "/", length(histsis), sep = ""))
        if (histsis[i] %in% nhh$hist) {
            mPhrase[i, ] <- Pboh(wis, histsis[i], k, ngh, nhh, fhf, n_words)
        } else {
            mPhrase[i, ] <- numeric(length(wis))
        }
    }
    mPhrase
}
predPhrase <- function (wis, hist) {
    hist <- tolower(hist)
    hist <- gsub("\\.", " ", hist)
    hist <- gsub(",", " ", hist)
    hist <- gsub("'", " ", hist)
    mPhrase <- predPhraseMatrix(wis, hist)
    sp <- strsplit(hist, " ")[[1]]
    histsis <- c()
    for (j in 1:length(sp)) {
        histsis <- c(histsis, Reduce(paste, sp[(length(sp) + 1 - j):length(sp)]))
    }
    histsis <- c("UNK", histsis)
    predMark(wis, histsis[histsis %in% nh$hist], mPhrase)
}

Pboih <- function (wi, hist) {
    n_words <- sum(ng$freq[ng$len == 1])
    sp <- strsplit(hist, " ")[[1]]
    histsis <- c()
    for (j in 1:length(sp)) {
        histsis <- c(histsis, Reduce(paste, sp[(length(sp) + 1 - j):length(sp)]))
    }
    histsis <- c("UNK", histsis)
    nhh <- nh[nh$hist %in% histsis, ]
    ngh <- ng[ng$hist %in% histsis, ]
    Pboh(wi, hist, k, ngh, nhh, fhf, n_words)
}
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
tfst_test <- termFreq(
    lines_test, 
    control = list(
        tokenize = function (x) NGramTokenizer(
            x, 
            control = Weka_control(min = 1, max = nmax)
        ),
        wordLengths=c(1, Inf)
    )
)
```

```{r, cache=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
# ng <- data.frame(ng=names(rs), freq = rs)
ng_test <- as.data.frame(tfst_test)
rm(tsft_test)
# ng <- as.data.frame(tfs)
names(ng_test) <- "freq"
ng_test$ng <- row.names(ng_test)
sp_test <- strsplit(as.character(ng_test$ng), " ")
ng_test$len <- sapply(sp_test, length)
```

```{r, cache=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
ng_test <- ng_test %>% arrange(-freq)
ngn_test <- ng_test %>% group_by(len) %>% summarise(n = n(), freq = sum(freq))
ng_test$perc <- ng_test$freq
for (i in 1:length(ngn_test$len)) {
    ng_test$perc[ng_test$len == ngn_test$len[i]] <- ng_test$perc[ng_test$len == ngn_test$len[i]] / (ngn_test$freq[i]**0.7) / (ngn_test$len[i]**1.3)
}
ng_test <- ng_test %>% arrange(-perc)

```

```{r, cache=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
sp_test <- strsplit(as.character(ng_test$ng), " ")
ng_test$len <- sapply(sp_test, length)
ng_test$hist <- sapply(sp_test, function(x) {
        if (length(x) == 1) {
            "UNK"
        } else {
            Reduce(paste, x[1:(length(x) - 1)])
        }
    }
)
ng_test$y <- sapply(sp_test, function(x) Reduce(paste, x[length(x):length(x)]))
rm(sp_test)
```

```{r, cache=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
nh_test <- ng_test %>% 
    select(hist, freq) %>%
    group_by(hist) %>%
    summarise(freq = sum(freq)) %>%
    arrange(-freq)
```

```{r, cache=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
fhf_test <- table(nh_test$freq)
```


Perplexity
```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
L <- 1000 # 483
ps <- sapply(1:L, function(i) Pboih(ng_test$y[i], ng_test$hist[i]))
sums <- log2(ps)
summ <- sum(sums)
perp <- 2**(-1/(length(ps)) * summ)
prodd <- prod(1/ps)
perp2 <- prodd**(1/length(ps))
```

Missed n-grams

```{r, cache=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
for (n in 1:4) {
    print(paste0("Most Common Missed ", n, " grams:"))
    ng_test[!(ng_test$ng %in% ng$ng) & ng_test$len == n, ] %>% head() %>% print()
}
```

Summary
```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
k <- 0
mlines
nmax
lowfreq
lowfreq1
lowfreqn
# sparsity
format(object.size(ng), units = "auto")
# tf
ngn
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 1 -pretzels -cheese -soda
wis <- c("cheese", "soda", "beer", "pretzels")
hist <- "The guy in front of me just bought a pound of bacon a bouquet and a case of"
# predictWord(wi, hist, k, ng, nh, fhf, hlen = 1:6)
# predMark(wi, hist, m)
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 2 world
wis <- c("universe", "world", "best", "most")
hist <- "You re the reason why I smile everyday Can you follow me please It would mean the"
# predictWord(wi, hist, k, ng, nh, fhf, hlen = 1:6)
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 3 happiest
wis <- c("smelliest", "bluest", "saddest", "happiest")
hist <- "Hey sunshine, can you follow me and make me the"
# predictWord(wi, hist, k, ng, nh, fhf, hlen = 1:6)
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 4 defense
wis <- c("crowd", "defense", "players", "referees")
hist <- "Very early observations on the Bills game: Offense still struggling but the"
# predictWord(wi, hist, k, ng, nh, fhf, hlen = 4)
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 5 beach
wis <- c("mall", "beach", "grocery", "movies")
hist <- "Go on a romantic date at the"
# predictWord(wi, hist, k, ng, nh, fhf, hlen = 4)
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 6 way
wis <- c("horse", "phone", "way", "motorcycle")
hist <- "Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my"
# predictWord(wi, hist, k, ng, nh, fhf, hlen = 4)
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 7 time
wis <- c("time", "years", "thing", "weeks")
hist <- "Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some"
# predictWord(wi, hist, k, ng, nh, fhf, hlen = 4)
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 8 fingers
wis <- c("eyes", "fingers", "ears", "toes")
hist <- "After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little"
# predictWord(wi, hist, k, ng, nh, fhf, hlen = 4)
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 9 bad
wis <- c("worse", "hard", "bad", "sad")
hist <- "Be grateful for the good times and keep the faith during the"
# predictWord(wi, hist, k, ng, nh, fhf, hlen = 4)
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 10 insane
wis <- c("callous", "insensitive", "insane", "asleep")
hist <- "If this isn't the cutest thing you've ever seen, then you must be"
# predictWord(wi, hist, k, ng, nh, fhf, hlen = 4)
predPhrase(wis, hist)
```


Task 5 - Creative Exploration
So far you have used basic models to understand and predict words. In this next task, your goal is to use all the resources you have available to you (from the Data Science Specialization, resources on the web, or your own creativity) to improve the predictive accuracy while reducing computational runtime and model complexity (if you can). Be sure to hold out a test set to evaluate the new, more creative models you are building.

Tasks to accomplish

1. Explore new models and data to improve your predictive model.
2. Evaluate your new predictions on both accuracy and efficiency. 
Questions to consider

1. What are some alternative data sets you could consider using? 
  Some online corpus
2. What are ways in which the n-gram model may be inefficient?
  If n is too big you have to manage a big model, which might be inefficient
3. What are the most commonly missed n-grams? 
Can you think of a reason why they would be missed and fix that? 
  Some n-grams are missed, this can be checked with the test set.
  One reason can be the length of the training set, so take a larger train set.
  Another reason can be the monotony of the training set, and in particular the 
  type of corpus (blogs, news and twitter). This can be fix adding extra data 
  sets
  The model will have a limited amount of n-grams so some of them will be 
  dropped. One way to fix it it's taking more n-grams in the model, but you have
  to consider the size of the model.
4. What are some other things that other people have tried to improve their model? 
  Improve the representation
  Optimize the code and the data formats, e.g. data.table instead of data.frames
  I optimize some part of the code to calculate the Pbo for a specific hist and 
  a list of different words (all the possibilities in the model at once).
  Get more data from different sources or read the all the corpus provided in 
  the course
5. Can you estimate how uncertain you are about the words you are predicting? 
  Given the probabilities, I can estimate the uncertainty comparing the 
  probabilities for the top predictions. For example, predicting the next world
  without knowing the history corresponds to the frequency of the word, so if 
  the frequencies are similar the uncertainty is high.
  
```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 1 die
wis <- c("die", "give", "sleep", "eat")
hist <- "When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd"
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 2 marital
wis <- c("spiritual", "horticultural", "financial", "marital")
hist <- "Guy at my table's wife got up to go to the bathroom and I asked about dessert and he started telling me about his"
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 3 weekend
wis <- c("weekend", "month", "morning", "decade")
hist <- "I'd give anything to see arctic monkeys this"
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 4 stress
wis <- c("stress", "hunger", "sleepiness", "happiness")
hist <- "Talking to your mom has the same effect as a hug and helps reduce your"
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 5 picture
wis <- c("picture", "walk", "look", "minute")
hist <- "When you were in Holland you were like 1 inch away from me but you hadn't time to take a"
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 6 matter
wis <- c("matter", "incident", "case", "account")
hist <- "I'd just like all of these questions answered, a presentation of evidence, and a jury to settle the"
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 7 hand
wis <- c("arm", "finger", "hand", "toe")
hist <- "I can't deal with unsymetrical things. I can't even hold an uneven number of bags of groceries in each"
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 8 top
wis <- c("side", "center", "middle", "top")
hist <- "Every inch of you is perfect from the bottom to the"
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 9 outside
wis <- c("daily", "inside", "outside", "weekly")
hist <- "I’m thankful my childhood was filled with imagination and bruises from playing"
predPhrase(wis, hist)
```

```{r, cache=TRUE, eval = FALSE, message=FALSE, warning=FALSE}
# 10 movies
wis <- c("pictures", "stories", "novels", "movies")
hist <- "I like how the same people are in almost all of Adam Sandler's"
predPhrase(wis, hist)
```
