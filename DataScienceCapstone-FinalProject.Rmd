---
title: "Data Science Capstone - Final Project"
author: "Daniel Contrera"
date: "12/29/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

In the following chunk we read each subset
```{r, cache = TRUE, eval = TRUE, message = FALSE, warning=FALSE}
filenames <- c(
    "Coursera-SwiftKey/train/en_US.twitter.txt",
    "Coursera-SwiftKey/train/en_US.blogs.txt",
    "Coursera-SwiftKey/train/en_US.news.txt"
)
lines_file <- c()
for (f in filenames) {
    con <- file(f, "r")
    max_lines <- 500
    lines_file <- c(lines_file, readLines(con, max_lines))
    close(con)
}
max_lines <- length(lines_file)
```


Then:

- keep the letters, `'` and spaces

- replace double spaces with a single one

- delete spaces at the beginning or end

- lowercase

```{r, cache = TRUE, eval = TRUE}
inds <- 1:max_lines
lines <- lines_file[inds]
lines[inds] <- gsub("((?![a-zA-Z' ]).)*", "", lines[inds], perl = TRUE)
lines[inds] <- gsub("  +", " ", lines[inds])
lines[inds] <- gsub("^ | $", "", lines[inds])
lines[inds] <- tolower(lines[inds])
```

## Prediction Model

```{r, eval = TRUE, message = FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
```

### n-gram

```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(tm)
library(RWeka)
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
text <- VCorpus(VectorSource(lines))
```


```{r nGramTokenizer, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
tdm <- TermDocumentMatrix(text, control=list(tokenize = function (x) NGramTokenizer(x, control = Weka_control(min = 1, max = 4))))
```

```{r, cache=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
ft <- findFreqTerms(tdm, lowfreq = 1)
rs <- rowSums(as.matrix(tdm[ft,]))
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
ng <- data.frame(ng=names(rs), freq = rs)

ng <- ng %>% arrange(-freq)
ng$perc <- ng$freq/(sum(ng$freq))
ng$perc_cum <- ng$perc
for (i in 2:length(ng$perc_cum)) {
    ng$perc_cum[i] <- ng$perc_cum[i - 1] + ng$perc_cum[i]
}
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
sp <- strsplit(as.character(ng$ng), " ")
ng$len <- sapply(sp, length)
ng$hist <- sapply(sp, function(x) Reduce(paste, x[1:(length(x) - 1)]))
ng$y <- sapply(sp, function(x) Reduce(paste, x[length(x):length(x)]))
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
nh <- ng %>% 
    select(hist, freq) %>%
    group_by(hist) %>%
    summarise(freq = sum(freq)) %>%
    arrange(-freq)
```

Discount

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
fhf <- table(nh$freq)
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# discount
d <- function (c, fhf) {
    nc <- fhf[as.character(c)]
    nc1 <- fhf[as.character(c + 1)]
    nc <- replace(nc, which(is.na(nc)), 1)
    nc1 <- replace(nc1, which(is.na(nc1)), 1)
    # count good turing
    cgt <- (c + 1) * nc1/nc
    ifelse(c == 0, fhf[1]/sum(fhf), cgt/c)
}
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# count ngram
c_hist <- function (hist, nh) {
    nh[nh$hist %in% hist, ]$freq
}
```


```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# beta
b <- function (hist, k, ng, nh, fhf) {
    words <- ng$y[ng$hist == hist]
    C <- ng$freq[ng$ng %in% paste(hist, words) & ng$freq > k]
    Ch <- nh$freq[nh$hist == hist]
    dc <- d(C, fhf)
    sums <- dc * C/Ch
    summ <- sum(sums)
    1 - summ
}
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
Pbo <- function (wi, hist, k, ng, nh, fhf) {
    Cwi <- ng$freq[ng$ng %in% paste(hist, wi) & ng$freq > k]
    Cwh <- ng$freq[ng$ng %in% paste(hist) & ng$freq > k]
    dw <- d(Cwi, fhf)
    ah <- a(hist, k, ng, nh, fhf)
    if (Cwi > k) {
        p <- dw * Cwi / Cwh
    } else {
        ws <- strsplit(hist, " ")[[1]]
        if (length(ws) > 1) {
            hist2 <- ws[2:length(ws)]
            pBack <- Pbo(wi, hist2, k, ng, nh, fhf)
        } else {
            n_words <- sum(ng$freq[ng$len == 1])
            if (is.na(ng$ng[ng$ng == wi])) {
                pBack <- dw * ng$ng[ng$ng == "UNK"]/n_words
            } else {
                pBack <- dw * ng$ng[ng$ng == wi]/n_words
            }
        }
        p <- ah * pBack
    }
    p
}
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# alpha
a <- function (hist, k, ng, nh, fhf) {
    beta <- b(hist, k, ng, nh, fhf)
    words <- ng$y[ng$hist == hist]
    ws <- strsplit(hist, " ")[[1]]
    if (is.list(ws) & length(ws) > 1) {
        hist2 <- ws[2:length(ws)]
        Pbos <- Pbo(wi, hist2, k, ng, nh, fhf)
    } else {
        if (wi %in% ng$ng) {
            Pbos <- ng$freq[ng$ng == wi]
        } else {
            Pbos <- ng$freq[ng$ng == "UNK"]
        }
    }
    summ <- sum(Pbos)
    beta/(1 - summ)
}
```

