---
title: "Data Science Capstone - Final Project"
author: "Daniel Contrera"
date: "12/29/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

In the following chunk we read each subset
```{r, cache = TRUE, eval = TRUE, message = FALSE, warning=FALSE}
filenames <- c(
    "Coursera-SwiftKey/train/en_US.twitter.txt",
    "Coursera-SwiftKey/train/en_US.blogs.txt",
    "Coursera-SwiftKey/train/en_US.news.txt"
)
lines_file <- c()
for (f in filenames) {
    con <- file(f, "r")
    max_lines <- 500
    lines_file <- c(lines_file, readLines(con, max_lines))
    close(con)
}
max_lines <- length(lines_file)
```


Then:

- keep the letters, `'` and spaces

- replace double spaces with a single one

- delete spaces at the beginning or end

- lowercase

```{r, cache = TRUE, eval = TRUE}
inds <- 1:max_lines
lines <- lines_file[inds]
lines[inds] <- gsub("((?![a-zA-Z' ]).)*", "", lines[inds], perl = TRUE)
lines[inds] <- gsub("  +", " ", lines[inds])
lines[inds] <- gsub("^ | $", "", lines[inds])
lines[inds] <- tolower(lines[inds])
```

## Prediction Model

```{r, eval = TRUE, message = FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
```

### n-gram

```{r, eval=TRUE, message=FALSE, warning=FALSE}
library(tm)
library(RWeka)
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
text <- VCorpus(VectorSource(lines))
```


```{r nGramTokenizer, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
tdm <- TermDocumentMatrix(
    text, 
    control=list(
        tokenize = function (x) NGramTokenizer(
            x, 
            control = Weka_control(min = 1, max = 4)
        ),
        wordLengths=c(1, Inf)
    )
)
```

```{r, cache=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
ft <- findFreqTerms(tdm, lowfreq = 1)
rs <- rowSums(as.matrix(tdm[ft,]))
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
ng <- data.frame(ng=names(rs), freq = rs)

ng <- ng %>% arrange(-freq)
ng$perc <- ng$freq/(sum(ng$freq))
ng$perc_cum <- ng$perc
for (i in 2:length(ng$perc_cum)) {
    ng$perc_cum[i] <- ng$perc_cum[i - 1] + ng$perc_cum[i]
}
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
sp <- strsplit(as.character(ng$ng), " ")
ng$len <- sapply(sp, length)
ng$hist <- sapply(sp, function(x) {
        if (length(x) == 1) {
            ""
        } else {
            Reduce(paste, x[1:(length(x) - 1)])
        }
    }
)
ng$y <- sapply(sp, function(x) Reduce(paste, x[length(x):length(x)]))
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
nh <- ng %>% 
    select(hist, freq) %>%
    group_by(hist) %>%
    summarise(freq = sum(freq)) %>%
    arrange(-freq)
```

Discount

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
fhf <- table(nh$freq)
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# discount
d <- function (c, fhf) {
    nc <- fhf[as.character(c)]
    nc1 <- fhf[as.character(c + 1)]
    nc <- replace(nc, which(is.na(nc)), 1)
    nc1 <- replace(nc1, which(is.na(nc1)), 1)
    # count good turing
    cgt <- (c + 1) * nc1/nc
    ifelse(c == 0, fhf[1]/sum(fhf), cgt/c)
}
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# count ngram
c_hist <- function (hist, nh) {
    nh[nh$hist %in% hist, ]$freq
}
```


```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# beta
b <- function (hist, k, ng, nh, fhf) {
    words <- ng$y[ng$hist == hist]
    C <- ng$freq[ng$ng %in% paste(hist, words) & ng$freq > k]
    Ch <- nh$freq[nh$hist == hist]
    dc <- d(C, fhf)
    sums <- dc * C/Ch
    summ <- sum(sums)
    1 - summ
}
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
Pbo <- function (wi, hist, k, ng, nh, fhf) {
    if (length(hist) == 1 & hist[1] == "") {
        ph <- wi
        Cwh <- ng$freq[ng$ng == paste(hist) & ng$freq > k]
    } else {
        ph <- paste(hist, wi)
        Cwh <- ng$freq[ng$ng == paste(hist) & ng$freq > k]
    }
    Cwi <- c()
    for (j in 1:length(ph)) {
        if (ph[j] %in% ng$ng) {
            Cwi <- c(Cwi, ng$freq[ng$ng == ph[j] & ng$freq > k])
        } else {
            Cwi <- c(Cwi, 0)
        }
    }
    dw <- d(Cwi, fhf)
    ah <- a(hist, k, ng, nh, fhf)
    p <- c()
    for (i in 1:length(wi)) {
        if (Cwi[i] > k & !(length(hist) == 1 & hist[1] == "")) {
            p <- c(p, dw[i] * Cwi[i] / Cwh)
        } else {
            ws <- strsplit(hist, " ")[[1]]
            if (length(ws) > 1) {
                hist2 <- ws[2:length(ws)]
                pBack <- Pbo(wi[i], hist2, k, ng, nh, fhf)
                p <- c(p, ah * pBack)
            } else {
                n_words <- sum(ng$freq[ng$len == 1])
                if (wi[i] %in% ng$ng) {
                    p <- c(p, dw[i] * ng$freq[ng$ng == wi[i]]/n_words)
                } else {
                    p <- c(p, dw[i] * ng$freq[ng$ng == "UNK"]/n_words)
                }
            }
        }
    }
    p
}
```

```{r, cache=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
# alpha
a <- function (hist, k, ng, nh, fhf) {
    beta <- b(hist, k, ng, nh, fhf)
    words <- ng$y[ng$hist == hist]
    ws <- strsplit(hist, " ")[[1]]
    if (is.list(ws) & length(ws) > 1) {
        hist2 <- ws[2:length(ws)]
        Pbos <- Pbo(words, hist2, k, ng, nh, fhf)
    } else {
        if (any(ng$ng %in% wi)) {
            Pbos <- ng$freq[ng$ng %in% wi]
        } else {
            Pbos <- ng$freq[ng$ng == "UNK"]
        }
    }
    summ <- sum(Pbos)
    beta/(1 - summ)
}
```

